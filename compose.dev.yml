services:

  vectordb:
    extends:
      file: compose.yml
      service: vectordb
    ports:
      - 6333:6333
      - 6334:6334

  api:
    extends:
      file: compose.yml
      service: api
    ports:
      - 8000:80
    volumes:
      - ./packages:/app/packages
    command: ["uvicorn", "src.expasy_agent.api:app", "--host", "0.0.0.0", "--port", "80", "--reload"]
    # command: ["uvicorn", "src.expasy_agent.api:app", "--host", "0.0.0.0", "--port", "80", "--http", "httptools"]
    # command: ["uvicorn", "src.expasy_agent.api:app", "--host", "0.0.0.0", "--port", "80", "--http", "h11"]
    # h11 written in python, httptools in C
    # entrypoint: /start-reload.sh --http h11

  # langgraph-redis:
  #   extends:
  #     file: compose.yml
  #     service: langgraph-redis

  # langgraph-postgres:
  #   extends:
  #     file: compose.yml
  #     service: langgraph-postgres
  #   ports:
  #     - 5433:5432

  # langgraph-api:
  #   extends:
  #     file: compose.yml
  #     service: langgraph-api
  #   ports:
  #     - 8123:8000

  # In case you need a GPU-enabled workspace
  # workspace:
  #   image: ghcr.io/vemonet/gpu-workspace:main
  #   # Enable GPUs in this container:
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: 1
  #   # Shared memory size for the container
  #   shm_size: '8g'
  #   volumes:
  #   - ./:/app
  #   # - /tmp/fastembed_cache/models--qdrant--bge-large-en-v1.5-onnx/:/tmp/fastembed_cache/models--qdrant--bge-large-en-v1.5-onnx/
  #   environment:
  #     TZ: Europe/Amsterdam
