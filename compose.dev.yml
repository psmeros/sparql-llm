services:

  vectordb:
    extends:
      file: compose.yml
      service: vectordb
    ports:
      - 6333:6333
      - 6334:6334

  api:
    extends:
      file: compose.yml
      service: api
    ports:
      - 8000:80
    volumes:
      - ./packages:/app/packages
    environment:
      - VECTORDB_URL=http://vectordb:6334/
      # - DEFAULT_LLM_MODEL=groq/deepseek-r1-distill-llama-70b
      # - DEFAULT_LLM_MODEL=groq/llama-3.3-70b-versatile
      # - DEFAULT_LLM_MODEL=deepseek/deepseek-reasoner
      # - DEFAULT_LLM_MODEL=deepseek/deepseek-chat
      # - DEFAULT_LLM_MODEL=azure/Mistral-Large-2411
      # - DEFAULT_LLM_MODEL=azure/DeepSeek-R1
      # - DEFAULT_LLM_MODEL=openai/o3-mini
      - DEFAULT_LLM_MODEL=openai/gpt-4o-mini
    entrypoint: ["uv", "run", "uvicorn", "src.expasy_agent.main:app", "--host", "0.0.0.0", "--port", "80", "--reload"]

  # langgraph-redis:
  #   extends:
  #     file: compose.yml
  #     service: langgraph-redis

  # langgraph-postgres:
  #   extends:
  #     file: compose.yml
  #     service: langgraph-postgres
  #   ports:
  #     - 5433:5432

  # langgraph-api:
  #   extends:
  #     file: compose.yml
  #     service: langgraph-api
  #   ports:
  #     - 8123:8000

  # In case you need a GPU-enabled workspace
  # workspace:
  #   image: ghcr.io/vemonet/gpu-workspace:main
  #   # Enable GPUs in this container:
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: 1
  #   # Shared memory size for the container
  #   shm_size: '8g'
  #   volumes:
  #   - ./:/app
  #   # - /tmp/fastembed_cache/models--qdrant--bge-large-en-v1.5-onnx/:/tmp/fastembed_cache/models--qdrant--bge-large-en-v1.5-onnx/
  #   environment:
  #     TZ: Europe/Amsterdam
